# 关卡编辑器增强版 - 快速上手指南

## 🚀 快速开始

### 1. 启用增强版编辑器

有两种方式启用新版本：

#### 方式A：替换现有版本（推荐）

编辑路由文件，将旧版本替换为增强版：

```typescript
// frontend/src/router/index.ts
import LevelEditorEnhanced from '../pages/teacher/LevelEditorEnhanced.vue'

{
  path: 'levels/:levelId/editor',
  name: 'teacher-level-editor',
  component: LevelEditorEnhanced, // 使用增强版
}
```

#### 方式B：保留两个版本

添加新路由，保留旧版本：

```typescript
// 旧版本
{
  path: 'levels/:levelId/editor',
  name: 'teacher-level-editor',
  component: LevelEditor,
},

// 新版本
{
  path: 'levels/:levelId/editor-v2',
  name: 'teacher-level-editor-v2',
  component: LevelEditorEnhanced,
}
```

然后在地图编辑器中修改跳转路由：

```typescript
// frontend/src/pages/teacher/LevelMapEditor.vue
const handleOpenLevelEditor = () => {
  // ...
  router.push({
    name: 'teacher-level-editor-v2', // 跳转到增强版
    params: { levelId: node.levelId }
  })
}
```

---

## ✨ 主要改进

### 1. Markdown 全面支持

所有文本输入框都支持 Markdown 格式：

**支持的语法：**
- 标题：`# H1`, `## H2`, `### H3`
- 粗体：`**粗体文本**`
- 斜体：`*斜体文本*`
- 代码：`` `行内代码` ``
- 代码块：
  ````
  ```python
  print("Hello")
  ```
  ````
- 列表：`- 列表项` 或 `1. 列表项`
- 链接：`[文字](url)`
- 图片：`![描述](url)`
- 引用：`> 引用文本`

**使用示例：**

```markdown
## 任务概述
本任务将带你学习如何使用 **Spark** 进行数据处理。

### 学习目标
- 掌握 Spark DataFrame API
- 了解分布式计算原理
- 完成数据清洗实战

### 示例代码
```python
df = spark.read.csv("data.csv")
df_clean = df.dropna()
```

> 注意：请确保已安装 Spark 3.0+
```

---

### 2. 三种编辑模式

Markdown 编辑器提供三种模式：

- **编辑模式** ✏️：专注编写
- **分屏模式** ⚏：边写边看
- **预览模式** 👁️：查看效果

**快捷键：**
- `Tab`：插入缩进
- `Ctrl/Cmd + B`：加粗
- `Ctrl/Cmd + I`：斜体

---

### 3. 卡片式任务展示

任务列表支持两种视图：

#### 网格视图 ⊞（默认）
```
┌────────┐ ┌────────┐ ┌────────┐
│ 任务1  │ │ 任务2  │ │ 任务3  │
│        │ │        │ │        │
└────────┘ └────────┘ └────────┘
```

#### 列表视图 ☰（紧凑）
```
─────────────────────────────
 任务1
─────────────────────────────
 任务2
─────────────────────────────
 任务3
─────────────────────────────
```

点击右上角切换按钮即可切换。

---

### 4. 响应式设计

#### 桌面端（> 1024px）
- 侧边栏固定显示
- 内容区域宽敞
- 完整功能

#### 平板端（640px - 1024px）
- 侧边栏变为抽屉
- 点击汉堡菜单打开/关闭
- 适配触摸操作

#### 移动端（< 640px）
- 按钮只显示图标
- 布局优化
- 单列显示

---

## 📝 使用流程

### 第一步：进入关卡编辑

1. 在关卡地图页面，点击任意关卡节点
2. 确保节点已保存（有 levelId）
3. 点击顶部工具栏的 **"关卡编辑"** 按钮
4. 进入增强版编辑器

---

### 第二步：编辑基本信息

1. 点击左侧 **"📝 基本信息"** 标签
2. 填写关卡名称（必填）
3. 使用 Markdown 编辑器编写关卡描述：
   - 点击工具栏按钮快速插入格式
   - 使用快捷键提高效率
   - 在右侧实时预览效果
4. 勾选"学生可见"选项
5. 点击 **"💾 保存基本信息"**

---

### 第三步：创建任务

#### 方式A：AI 生成（推荐）

1. 点击 **"✅ 任务配置"** 标签
2. 在 AI 生成面板中输入需求：
   ```
   生成一个关于 Spark 数据清洗的任务，
   包含数据加载、缺失值处理、异常值检测等步骤
   ```
3. 点击 **"🤖 AI 生成"**
4. 查看生成结果（支持 Markdown 预览）
5. 点击 **"✓ 接受"** 创建任务

#### 方式B：手动创建

1. 点击 **"+ 新建任务"**
2. 填写任务名称
3. 使用 Markdown 编辑器编写：
   - 任务描述：详细说明任务内容
   - 任务目标：列出学习目标
4. 点击 **"💾 保存任务"**

**提示：** 可以在任务描述中使用代码块展示示例代码！

---

### 第四步：配置学习卡片

1. 在任务列表中**选中一个任务**（点击卡片）
2. 点击 **"📚 学习卡片"** 标签
3. 使用 AI 生成或手动添加：
   - **知识卡片**：理论知识、概念讲解
   - **技能卡片**：实操技能、工具使用
4. 卡片内容支持 Markdown 格式

---

### 第五步：设计操作步骤

1. 确保已选中任务
2. 点击 **"🔄 环节步骤"** 标签
3. 使用 AI 自动生成或手动配置：
   - **环节**：任务的大阶段（如：准备、实施、验收）
   - **步骤**：环节中的具体操作
4. 配置每个步骤的：
   - 步骤名称和内容（支持 Markdown）
   - 提交类型（文本/文件/链接/代码）
   - 操作要求

---

### 第六步：添加考题

1. 点击 **"📋 闯关考题"** 标签
2. 使用 AI 生成或手动创建：
   ```
   AI 指令示例：
   生成5道关于 Spark 的单选题，
   难度为中等，涵盖 DataFrame API
   ```
3. 设置题目的：
   - 题型（单选/多选/判断/简答）
   - 难度等级
   - 分值
   - 知识点标签
4. 题目内容支持 Markdown（可插入代码）

---

### 第七步：发布关卡

1. 检查完成度进度条（左下角）
2. 确认所有内容配置完整
3. 点击顶部 **"📢 发布"** 按钮
4. 关卡状态变为"已发布"，学生可见

---

## 💡 使用技巧

### 1. Markdown 实用技巧

#### 插入代码示例
````markdown
```python
# 数据加载
df = spark.read.csv("data.csv", header=True)

# 数据清洗
df_clean = df.dropna().drop_duplicates()
```
````

#### 制作任务清单
```markdown
## 任务步骤

- [ ] 安装 Spark 环境
- [ ] 下载数据集
- [ ] 编写清洗脚本
- [ ] 运行并验证结果
```

#### 添加提示框
```markdown
> 💡 **提示**：建议先在小数据集上测试代码

> ⚠️ **注意**：确保 Spark 版本 >= 3.0
```

---

### 2. AI 生成优化

#### 写好 AI 指令

❌ **不好的指令：**
```
生成一个任务
```

✅ **好的指令：**
```
生成一个大数据实战任务，主题是使用 Spark 进行电商数据分析。
任务应该包含：
1. 数据加载和预处理
2. 销售趋势分析
3. 用户行为分析
4. 可视化展示

目标学生：有 Python 基础的大数据初学者
```

#### 利用 AI 生成 Markdown

AI 生成的内容会自动包含 Markdown 格式，你可以：
- 接受后查看渲染效果
- 根据需要微调格式
- 补充代码示例

---

### 3. 响应式布局技巧

#### 移动端使用

1. 点击左上角 **☰** 打开侧边栏
2. 选择要编辑的标签
3. 侧边栏自动关闭，专注内容编辑
4. 支持手势滑动操作

#### 全屏编辑

1. 在 Markdown 编辑器中点击 **全屏** 按钮
2. 进入无干扰编辑模式
3. 按 `Esc` 退出全屏

---

### 4. 工作流程建议

#### 推荐流程（从上到下）

```
1. 基本信息  →  定义关卡目标
2. 任务配置  →  规划学习任务
3. 学习卡片  →  准备学习材料
4. 环节步骤  →  设计操作流程
5. 闯关考题  →  设置考核标准
6. 发布关卡  →  上线供学生使用
```

#### AI 辅助流程

```
1. AI 生成任务（粗框架）
2. 手动完善描述（加细节）
3. AI 生成卡片（理论知识）
4. AI 生成步骤（操作流程）
5. AI 生成考题（知识检验）
6. 最终检查优化
```

---

## 🎯 最佳实践

### 1. 编写任务描述

```markdown
## 任务概述
本任务将通过实战项目，帮助你掌握 Spark 数据处理核心技能。

## 背景说明
某电商公司需要分析最近一个月的销售数据，
发现商品热销规律并优化库存策略。

## 技术栈
- Apache Spark 3.2+
- Python 3.8+
- Pandas、Matplotlib

## 数据集
- sales_data.csv (100MB)
- 包含：订单ID、商品信息、销售额、时间戳

## 预期成果
1. 清洗后的数据集
2. 销售趋势分析报告
3. 可视化图表（至少3张）
4. 库存优化建议

## 评分标准
- 数据清洗质量（30分）
- 分析深度（40分）
- 可视化效果（20分）
- 报告完整性（10分）
```

---

### 2. 设计学习卡片

#### 知识卡片示例

```markdown
### 📚 Spark DataFrame 基础

#### 什么是 DataFrame？
DataFrame 是 Spark 中用于处理结构化数据的核心抽象，
类似于关系数据库中的表或 Pandas 中的 DataFrame。

#### 主要特点
- **分布式计算**：自动并行处理大数据
- **惰性执行**：只在需要时才计算结果
- **类型安全**：编译时检查数据类型

#### 常用操作
```python
# 读取数据
df = spark.read.csv("data.csv")

# 数据筛选
df.filter(df.age > 18)

# 数据聚合
df.groupBy("city").count()
```

#### 学习资源
- [官方文档](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [在线教程](https://example.com/spark-tutorial)
```

---

### 3. 规划操作步骤

**环节1：环境准备**
- 步骤1：安装 Spark 环境
- 步骤2：配置 Python 环境
- 步骤3：下载数据集

**环节2：数据清洗**
- 步骤1：加载数据并检查
- 步骤2：处理缺失值
- 步骤3：处理异常值
- 步骤4：数据类型转换

**环节3：数据分析**
- 步骤1：销售趋势分析
- 步骤2：商品排名分析
- 步骤3：时间序列分析

**环节4：结果展示**
- 步骤1：生成可视化图表
- 步骤2：编写分析报告
- 步骤3：提交成果

---

## ❓ 常见问题

### Q1: Markdown 不能正常渲染？

**A:** 检查语法是否正确：
- 标题符号后要有空格：`# 标题` ✅ / `#标题` ❌
- 代码块要用三个反引号包围
- 列表符号后要有空格

---

### Q2: AI 生成的内容不理想？

**A:** 优化你的指令：
1. 提供更详细的背景信息
2. 明确目标学生群体
3. 指定期望的内容结构
4. 可以多次生成并选择最佳结果

---

### Q3: 移动端侧边栏无法关闭？

**A:** 点击内容区域任意位置即可关闭侧边栏，或点击菜单按钮切换。

---

### Q4: 如何插入图片？

**A:** 两种方式：
1. 使用 Markdown 语法：`![描述](图片URL)`
2. 未来版本会支持直接上传图片

---

### Q5: 能否导出/导入内容？

**A:** 当前版本暂不支持，该功能在规划中。
建议先复制 Markdown 内容到本地保存。

---

## 📞 获取帮助

如果遇到问题或有建议，请：

1. 查看完整设计文档：`doc/关卡编辑器增强版设计文档.md`
2. 查看代码注释和示例
3. 提交 Issue 或联系开发团队

---

**祝你使用愉快！** 🎉
